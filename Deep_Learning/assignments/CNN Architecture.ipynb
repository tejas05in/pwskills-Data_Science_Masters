{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`CNN Architecture`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TOPIC: Understanding Pooling and Padding in CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describe the purpose and benifits of pooling in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling, in the context of Convolutional Neural Networks (CNNs), serves several crucial purposes and brings about specific benefits:\n",
    "\n",
    "### Purpose of Pooling:\n",
    "1. **Dimensionality Reduction:** Pooling reduces the spatial dimensions (width and height) of the input volume. This helps in controlling the number of parameters and computational complexity in subsequent layers, preventing overfitting and reducing computational cost.\n",
    "   \n",
    "2. **Feature Invariance:** Pooling helps create spatial invariance by making the network less sensitive to small variations or translations in the input data. This means the network can recognize the same features regardless of their position in the image.\n",
    "\n",
    "### Benefits of Pooling:\n",
    "1. **Translation Invariance:** Pooling creates translation invariance, meaning the network can recognize patterns or features regardless of their exact location in the input image. This is beneficial as it enables the network to generalize better across different positions of the same feature.\n",
    "\n",
    "2. **Reduced Overfitting:** By reducing the spatial dimensions, pooling helps in reducing overfitting by controlling the number of parameters. It extracts the most important information while discarding the less relevant details.\n",
    "\n",
    "3. **Computational Efficiency:** Pooling reduces the spatial size of the representation, thereby reducing the computational requirements in the network. This allows deeper networks to be constructed without excessively increasing the computational burden.\n",
    "\n",
    "### Types of Pooling:\n",
    "1. **Max Pooling:** Takes the maximum value from each window of the input. It retains the most activated features within each window.\n",
    "   \n",
    "2. **Average Pooling:** Computes the average value of each window in the input. It provides a more smoothed representation of the input.\n",
    "\n",
    "3. **Global Average Pooling:** Takes the average of each feature map across its entire spatial dimensions, resulting in a single value per feature map.\n",
    "\n",
    "In summary, pooling in CNNs plays a crucial role in reducing dimensionality, creating spatial invariance, enhancing computational efficiency, and aiding in preventing overfitting by summarizing essential information from feature maps. Max and average pooling are the most commonly used types, each with its advantages in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain the difference between min pooling and max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! The primary difference between min pooling and max pooling lies in how they operate within Convolutional Neural Networks (CNNs) to downsample feature maps:\n",
    "\n",
    "### Max Pooling:\n",
    "- **Operation:** Max pooling involves taking the maximum value within each window (typically non-overlapping) of the input feature map.\n",
    "- **Function:** It retains the most activated or prominent features within each pooling window.\n",
    "- **Benefit:** Max pooling emphasizes the most important features, making it particularly useful for retaining and highlighting the most significant activation in the feature map.\n",
    "- **Example:** In a 2x2 max pooling operation, for instance, the maximum value within each 2x2 window of the input feature map is retained.\n",
    "\n",
    "### Min Pooling:\n",
    "- **Operation:** Min pooling, on the other hand, involves taking the minimum value within each pooling window of the input feature map.\n",
    "- **Function:** It highlights the least activated or lowest-valued features within each window.\n",
    "- **Benefit:** Min pooling can be useful for certain applications where the presence of the smallest values is significant or when emphasizing low-intensity features is desirable.\n",
    "- **Example:** In a 2x2 min pooling operation, the minimum value within each 2x2 window of the input feature map is retained.\n",
    "\n",
    "### Key Differences:\n",
    "1. **Function:** Max pooling retains the maximum values (most activated features), while min pooling retains the minimum values (least activated features) within their respective windows.\n",
    "  \n",
    "2. **Application:** Max pooling is more commonly used and tends to emphasize prominent features, whereas min pooling may be used in specific scenarios where the emphasis is on detecting low-intensity features or outliers.\n",
    "\n",
    "3. **Effect:** Max pooling tends to enhance and emphasize salient features, while min pooling can highlight the least prominent features.\n",
    "\n",
    "In practice, max pooling is more prevalent in CNN architectures due to its effectiveness in retaining important features and aiding in translation invariance, though the choice between max pooling and min pooling can depend on the specific requirements and characteristics of the given task or dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Convolutional Neural Networks (CNNs), padding refers to the process of adding extra layers of pixels around the input image or feature map before applying convolution operations. This technique involves adding zeros or other values around the input data, effectively increasing its spatial dimensions. Padding plays a significant role in CNNs and holds several key significances:\n",
    "\n",
    "### Significance of Padding in CNNs:\n",
    "\n",
    "1. **Preservation of Spatial Information:**\n",
    "   - **Preventing Information Loss:** Without padding, as convolution operations are applied, the spatial dimensions of the feature maps decrease. This reduction can lead to loss of information, especially at the edges of the image.\n",
    "   - **Preserving Output Size:** Padding allows the convolutional layers to maintain the spatial dimensions of the input image or feature map throughout the convolution process, ensuring that the output size remains similar to the input size.\n",
    "\n",
    "2. **Enabling Effective Feature Extraction:**\n",
    "   - **Border Information Utilization:** Padding ensures that the information at the borders of the image is adequately considered during the convolution process. This is crucial for effectively extracting features from the entire input, including edge and corner information.\n",
    "\n",
    "3. **Mitigating Border Effects:**\n",
    "   - **Addressing Edge Effects:** Padding helps to alleviate border effects that occur due to the reduction in spatial dimensions during convolution. It allows the network to focus equally on all parts of the input image, preventing the network from giving less importance to the edges.\n",
    "\n",
    "4. **Facilitating Network Design and Flexibility:**\n",
    "   - **Control over Output Size:** By using padding, data scientists and neural network architects can control the spatial dimensions of the output feature maps after convolution operations. This control is crucial when designing the architecture of the network and helps in achieving the desired output size.\n",
    "   \n",
    "5. **Compatibility with Stride and Filter Size:**\n",
    "   - **Interaction with Stride:** Padding can interact with the stride (the amount by which the filter slides over the input) to influence the spatial dimensions of the output feature maps. It ensures compatibility between stride values, filter sizes, and input dimensions to generate desired output sizes.\n",
    "\n",
    "### Types of Padding:\n",
    "- **Valid (No Padding):** No additional padding is added, leading to a reduction in spatial dimensions.\n",
    "- **Same Padding:** Padding is added in such a way that the output spatial dimensions are the same as the input dimensions.\n",
    "\n",
    "In summary, padding in CNNs is crucial for maintaining spatial information, preventing information loss at the edges, enabling effective feature extraction, mitigating border effects, providing flexibility in network design, and ensuring compatibility with filter sizes and strides. It plays a pivotal role in achieving better performance and accuracy in convolutional operations within neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Zero-padding and valid-padding are two types of padding used in Convolutional Neural Networks (CNNs) that have contrasting effects on the output feature map size.\n",
    "\n",
    "### Zero-padding:\n",
    "- **Description:** Zero-padding involves adding extra rows and columns of zeros around the input feature map before applying the convolution operation.\n",
    "- **Effect on Output Size:** When using zero-padding, the size of the output feature map can be preserved or controlled.\n",
    "- **Maintaining Output Size:** Zero-padding ensures that the output size after convolution remains the same as the input size when using appropriate padding.\n",
    "- **Example:** If a 3x3 filter is convolved with a 5x5 input feature map and zero-padding of size 1 (adding one layer of zeros around the input), the resulting feature map will also be 5x5 in size.\n",
    "\n",
    "### Valid-padding:\n",
    "- **Description:** Valid-padding, also known as 'no padding,' involves applying the convolution operation without adding any extra borders or padding to the input feature map.\n",
    "- **Effect on Output Size:** With valid-padding, the output feature map size will be smaller than the input size.\n",
    "- **Reduction in Output Size:** Convolution with valid-padding results in a reduction of the output size compared to the input size.\n",
    "- **Example:** If a 3x3 filter is convolved with a 5x5 input feature map without any padding, the resulting feature map will be 3x3 in size (assuming a stride of 1).\n",
    "\n",
    "### Comparison in terms of Output Feature Map Size:\n",
    "\n",
    "- **Zero-padding:** Preserves the output feature map size or allows control over the output size by adding zeros around the input.\n",
    "- **Valid-padding:** Reduces the output feature map size compared to the input due to the absence of any extra padding.\n",
    "\n",
    "### Summary:\n",
    "- Zero-padding maintains the input dimensions in the output feature map or allows adjustment to maintain specific output sizes.\n",
    "- Valid-padding, on the other hand, reduces the output feature map size, as it does not add any extra borders or padding to the input.\n",
    "\n",
    "In practical CNN architectures, padding choices depend on the desired output size, the network's architecture, the intended information retention, and the spatial dimension requirements at different layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TOPIC: Exploring LeNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Provide a brief overview of the LeNet-5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner in the late 1990s. It was primarily designed for handwritten digit recognition and was one of the first successful applications of CNNs in the field of computer vision.\n",
    "\n",
    "### Overview of the LeNet-5 architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - Accepts grayscale images of size 32x32 pixels.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   - Layer C1: Convolutional layer with 6 feature maps, each using a 5x5 kernel and a stride of 1.\n",
    "   - Activation function: Sigmoid.\n",
    "   - Outputs: 28x28 feature maps.\n",
    "\n",
    "3. **Subsampling (Pooling) Layers:**\n",
    "   - Layer S2: Subsampling layer with 6 feature maps, each using 2x2 pooling with a stride of 2.\n",
    "   - Reduction in spatial dimensions to 14x14.\n",
    "\n",
    "4. **Convolutional Layers:**\n",
    "   - Layer C3: Convolutional layer with 16 feature maps, each using a 5x5 kernel applied to the 6 feature maps from the previous layer.\n",
    "   - Activation function: Sigmoid.\n",
    "   - Outputs: 10x10 feature maps.\n",
    "\n",
    "5. **Subsampling (Pooling) Layers:**\n",
    "   - Layer S4: Subsampling layer with 16 feature maps, each using 2x2 pooling with a stride of 2.\n",
    "   - Reduction in spatial dimensions to 5x5.\n",
    "\n",
    "6. **Fully Connected Layers:**\n",
    "   - Layer C5: Fully connected layer with 120 units.\n",
    "   - Activation function: Sigmoid.\n",
    "\n",
    "7. **Fully Connected Layers:**\n",
    "   - Layer F6: Fully connected layer with 84 units.\n",
    "   - Activation function: Sigmoid.\n",
    "\n",
    "8. **Output Layer:**\n",
    "   - Output layer with 10 units (corresponding to 10 digits in the dataset, e.g., digits 0 to 9 for MNIST).\n",
    "   - Activation function: Softmax.\n",
    "\n",
    "### Key Points:\n",
    "- LeNet-5 was groundbreaking at the time for its architecture in CNNs, introducing the concept of using convolutional layers followed by subsampling (pooling) layers.\n",
    "- It utilized sigmoid activation functions in the hidden layers.\n",
    "- The architecture aimed at reducing the spatial dimensions gradually while increasing the number of feature maps through convolution and subsampling layers.\n",
    "- LeNet-5 demonstrated impressive performance on digit recognition tasks, particularly on the MNIST dataset, contributing significantly to the development and popularization of CNNs in computer vision.\n",
    "\n",
    "Though LeNet-5 was introduced years ago, its fundamental design principles and concepts remain influential in the development of modern CNN architectures used for various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe the key components of LeNet-5 and their respective purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5, a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun and colleagues, consists of several key components, each serving a specific purpose in the network's design for handwritten digit recognition:\n",
    "\n",
    "### 1. Convolutional Layers:\n",
    "- **Purpose:** Extract meaningful features from input images through convolutions with learnable kernels.\n",
    "- **Key Details:** LeNet-5 has two convolutional layers:\n",
    "  - **Layer C1:** Contains 6 feature maps, each generated by convolving a 5x5 kernel with the input image.\n",
    "  - **Layer C3:** Consists of 16 feature maps obtained by convolving 5x5 kernels with the output of the previous layer.\n",
    "\n",
    "### 2. Subsampling (Pooling) Layers:\n",
    "- **Purpose:** Reduce spatial dimensions, abstracting the most essential information while retaining translational invariance.\n",
    "- **Key Details:** LeNet-5 includes two subsampling layers:\n",
    "  - **Layer S2:** Employs 2x2 max pooling over 6 feature maps to reduce dimensionality by half.\n",
    "  - **Layer S4:** Utilizes 2x2 max pooling over 16 feature maps, again reducing the spatial dimensions.\n",
    "\n",
    "### 3. Fully Connected Layers:\n",
    "- **Purpose:** Perform high-level reasoning and classification based on extracted features.\n",
    "- **Key Details:** LeNet-5 contains two fully connected layers:\n",
    "  - **Layer C5:** Consists of 120 units, connecting to the previous layers to perform further feature extraction.\n",
    "  - **Layer F6:** Contains 84 units, serving as a penultimate layer for higher-level feature representation.\n",
    "\n",
    "### 4. Activation Functions (Sigmoid and Softmax):\n",
    "- **Purpose:** Introduce non-linearity and perform classification.\n",
    "- **Key Details:** \n",
    "  - **Sigmoid Activation:** Used in layers C1, C3, C5, and F6 to introduce non-linearity.\n",
    "  - **Softmax Activation:** Applied in the output layer to produce probabilities for classifying digits (10 classes for digits 0-9).\n",
    "\n",
    "### 5. Output Layer:\n",
    "- **Purpose:** Generate the final classification probabilities.\n",
    "- **Key Details:** Consists of 10 output units (for 10 classes of digits), each representing the probability of an input belonging to a specific digit class.\n",
    "\n",
    "### Overall Functionality:\n",
    "- **Feature Extraction:** Convolutional and pooling layers gradually extract hierarchical features from the input images.\n",
    "- **Dimensionality Reduction:** Subsampling layers reduce spatial dimensions while retaining important information.\n",
    "- **Classification:** Fully connected layers and the output layer perform classification based on the extracted features.\n",
    "\n",
    "LeNet-5's key components and their interconnections, combining convolution, pooling, and fully connected layers with specific activation functions, formed the foundation for modern CNN architectures. This model showcased the power of deep learning in image recognition tasks, particularly handwritten digit recognition, and laid the groundwork for subsequent advancements in the field of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5, as one of the pioneering Convolutional Neural Network (CNN) architectures, introduced several advantages that were influential in the field of image classification. However, it also had certain limitations, especially when compared to more recent and sophisticated CNN models. Here's an overview of the advantages and limitations of LeNet-5:\n",
    "\n",
    "### Advantages of LeNet-5:\n",
    "\n",
    "1. **Effective Feature Extraction:**\n",
    "   - LeNet-5 demonstrated the capability of CNNs to effectively extract hierarchical features from images using convolutional and subsampling layers.\n",
    "\n",
    "2. **Pioneering Architecture:**\n",
    "   - It introduced the concept of alternating convolutional and subsampling layers, forming a basic blueprint for modern CNNs.\n",
    "\n",
    "3. **Robustness to Variations:**\n",
    "   - LeNet-5 showed reasonable robustness to variations in handwritten digits' shapes, sizes, and orientations, making it suitable for digit recognition tasks.\n",
    "\n",
    "4. **First Successful Application:**\n",
    "   - It was the first successful application of CNNs for image classification, specifically on the MNIST dataset, achieving remarkable accuracy at the time.\n",
    "\n",
    "5. **Influence on Future Architectures:**\n",
    "   - The architecture of LeNet-5 influenced the development of subsequent CNN models, guiding researchers in designing deeper and more complex networks.\n",
    "\n",
    "### Limitations of LeNet-5:\n",
    "\n",
    "1. **Limited Capacity and Depth:**\n",
    "   - LeNet-5's architecture is relatively shallow compared to modern CNNs. Its capacity to learn complex features might be limited for more intricate datasets and tasks.\n",
    "\n",
    "2. **Sigmoid Activation Function:**\n",
    "   - The use of sigmoid activation functions throughout the network can lead to issues like vanishing gradients, slowing down the learning process, and hindering training deeper networks.\n",
    "\n",
    "3. **Performance on Complex Datasets:**\n",
    "   - While effective for handwritten digit recognition, LeNet-5 might not perform optimally on more complex datasets with varied object categories and intricate visual features.\n",
    "\n",
    "4. **Lack of Non-linearity:**\n",
    "   - The use of sigmoid activations restricts the model's ability to capture non-linear relationships present in more diverse datasets.\n",
    "\n",
    "5. **Reduced Relevance in Modern Context:**\n",
    "   - The advancements in deep learning and CNN architectures have led to more sophisticated models (e.g., ResNet, Inception, etc.) that significantly surpass LeNet-5 in accuracy and efficiency.\n",
    "\n",
    "### Conclusion:\n",
    "LeNet-5 served as a foundational model, proving the efficacy of CNNs in image classification. While it had several advantages and contributed to shaping the future of CNN architectures, its limitations in capacity, depth, activation functions, and performance on complex datasets highlight the need for more advanced architectures to tackle modern computer vision challenges effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement LeNet-5 using a deep learning framework of your choice (Eg. TensorFlow or Pytorch) and train in a publicly available dataset (e.g.MINST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\pwskills-Data_Science_Masters\\env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the images to (num_samples, height, width, channels)\n",
    "train_images = train_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\pwskills-Data_Science_Masters\\env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Python\\pwskills-Data_Science_Masters\\env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Python\\pwskills-Data_Science_Masters\\env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 6)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44426 (173.54 KB)\n",
      "Trainable params: 44426 (173.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define LeNet-5 architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\Python\\pwskills-Data_Science_Masters\\env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Python\\pwskills-Data_Science_Masters\\env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.3137 - accuracy: 0.9107 - val_loss: 0.0873 - val_accuracy: 0.9731\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0816 - accuracy: 0.9756 - val_loss: 0.0569 - val_accuracy: 0.9811\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0605 - accuracy: 0.9815 - val_loss: 0.0548 - val_accuracy: 0.9816\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0466 - accuracy: 0.9857 - val_loss: 0.0414 - val_accuracy: 0.9862\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0381 - accuracy: 0.9882 - val_loss: 0.0515 - val_accuracy: 0.9835\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0322 - accuracy: 0.9897 - val_loss: 0.0399 - val_accuracy: 0.9870\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.0355 - val_accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0235 - accuracy: 0.9924 - val_loss: 0.0407 - val_accuracy: 0.9873\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.0365 - val_accuracy: 0.9887\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.0433 - val_accuracy: 0.9871\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0433 - accuracy: 0.9871\n",
      "Test accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "# Print test accuracy\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Insights**:\n",
    "1. Training Time: LeNet-5 trains relatively quickly on the MNIST dataset due to its simplicity.\n",
    "2. Accuracy: It should achieve decent accuracy (98.71%) on the MNIST dataset as it's well-suited for digit recognition tasks.\n",
    "3. Model Complexity: LeNet-5 is a shallow architecture compared to modern CNNs, which might limit its performance on more complex datasets.\n",
    "\n",
    "For practical purposes, you can run this code in a Python environment with TensorFlow installed to train and evaluate LeNet-5 on the MNIST dataset. Adjustments to hyperparameters or data preprocessing might further optimize its performance. Additionally, considering the model's limitations, you might observe challenges if applied to more complex datasets requiring deeper architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TOPIC: Analyzing AlexNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Present an overview of the AlexNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet is a pioneering convolutional neural network (CNN) architecture developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It gained significant attention by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "### Overview of the AlexNet Architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - Accepts RGB images of size 224x224 pixels.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   - **Convolutional Layer 1 (Conv1):**\n",
    "     - 96 kernels of size 11x11 with a stride of 4.\n",
    "     - Activation: Rectified Linear Unit (ReLU).\n",
    "     - Local Response Normalization (LRN) applied.\n",
    "   - **Convolutional Layer 2 (Conv2):**\n",
    "     - 256 kernels of size 5x5.\n",
    "     - Stride of 1.\n",
    "     - Activation: ReLU.\n",
    "     - LRN applied.\n",
    "\n",
    "3. **Subsampling (Pooling) Layers:**\n",
    "   - **Max Pooling Layer 1 (MaxPool1):**\n",
    "     - Size 3x3 with a stride of 2.\n",
    "   - **Max Pooling Layer 2 (MaxPool2):**\n",
    "     - Size 3x3 with a stride of 2.\n",
    "\n",
    "4. **Convolutional Layers:**\n",
    "   - **Convolutional Layer 3 (Conv3):**\n",
    "     - 384 kernels of size 3x3.\n",
    "     - Stride of 1.\n",
    "     - Activation: ReLU.\n",
    "   - **Convolutional Layer 4 (Conv4):**\n",
    "     - 384 kernels of size 3x3.\n",
    "     - Stride of 1.\n",
    "     - Activation: ReLU.\n",
    "   - **Convolutional Layer 5 (Conv5):**\n",
    "     - 256 kernels of size 3x3.\n",
    "     - Stride of 1.\n",
    "     - Activation: ReLU.\n",
    "   - **Max Pooling Layer 3 (MaxPool3):**\n",
    "     - Size 3x3 with a stride of 2.\n",
    "\n",
    "5. **Fully Connected Layers:**\n",
    "   - **Fully Connected Layer 1 (FC6):**\n",
    "     - 4096 neurons.\n",
    "     - Activation: ReLU.\n",
    "     - Dropout with a probability of 0.5 applied for regularization.\n",
    "   - **Fully Connected Layer 2 (FC7):**\n",
    "     - 4096 neurons.\n",
    "     - Activation: ReLU.\n",
    "     - Dropout with a probability of 0.5 applied for regularization.\n",
    "   - **Output Layer:**\n",
    "     - Fully Connected Layer (FC8) with 1000 neurons corresponding to the 1000 classes in the ImageNet dataset.\n",
    "     - Activation: Softmax.\n",
    "\n",
    "### Key Aspects of AlexNet:\n",
    "\n",
    "- **Deep Architecture:** AlexNet was one of the first deep CNNs with eight learned layers (five convolutional and three fully connected).\n",
    "- **ReLU Activation:** It used rectified linear units (ReLU) as activation functions, which mitigated the vanishing gradient problem and accelerated convergence.\n",
    "- **Local Response Normalization (LRN):** Applied to the first and second convolutional layers, promoting local contrast normalization.\n",
    "- **Dropout:** Used in fully connected layers for regularization to prevent overfitting.\n",
    "- **Parallel Computation:** Utilized two GPUs for simultaneous computation, pioneering the use of parallelism in CNNs.\n",
    "\n",
    "### Impact and Significance:\n",
    "\n",
    "- AlexNet significantly improved image classification accuracy on the ImageNet dataset, marking a breakthrough in the field of computer vision.\n",
    "- Its success popularized deep learning and CNNs, inspiring the development of more complex architectures and contributing to the resurgence of artificial neural networks in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explaint the architectural innovations introduced in AlexNet that contributed to its breakthrough performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet's breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) can be attributed to several architectural innovations that significantly improved the accuracy of image classification tasks. These innovations were pivotal in enhancing the model's performance and pushing the boundaries of deep learning. Here are the key architectural innovations introduced in AlexNet:\n",
    "\n",
    "### 1. Depth and Width:\n",
    "- **Deep Architecture:** AlexNet was one of the first CNNs to introduce a significantly deep architecture with eight learned layers (five convolutional and three fully connected layers). This depth allowed the network to learn hierarchical features at different levels of abstraction.\n",
    "\n",
    "### 2. Convolutional Layers:\n",
    "- **Large Convolutional Kernels:** It used large convolutional kernels, such as 11x11 and 5x5, in the initial layers (Conv1 and Conv2). This helped in capturing larger spatial features efficiently.\n",
    "- **Multiple Convolutional Layers:** Utilizing multiple convolutional layers with different kernel sizes and depths (e.g., Conv3, Conv4, Conv5) enabled the network to learn complex and abstract features.\n",
    "\n",
    "### 3. Activation Functions:\n",
    "- **Rectified Linear Units (ReLU):** AlexNet employed the ReLU activation function instead of traditional sigmoid or tanh activations. ReLU significantly accelerated the convergence of the network by mitigating the vanishing gradient problem, allowing for faster learning.\n",
    "\n",
    "### 4. Local Response Normalization (LRN):\n",
    "- **Local Response Normalization:** LRN was applied after the first and second convolutional layers (Conv1 and Conv2). This form of normalization helped enhance the contrast between local neighboring pixels, facilitating better feature extraction.\n",
    "\n",
    "### 5. Pooling Layers:\n",
    "- **Max Pooling:** Employed max pooling layers with a size of 3x3 and a stride of 2 (MaxPool1, MaxPool2, MaxPool3), which helped in reducing spatial dimensions while preserving important features.\n",
    "\n",
    "### 6. Fully Connected Layers:\n",
    "- **Large Fully Connected Layers:** AlexNet had two fully connected layers (FC6, FC7) with 4096 neurons each. These layers contributed to high-level reasoning and abstraction, capturing intricate relationships in the data.\n",
    "\n",
    "### 7. Regularization Techniques:\n",
    "- **Dropout Regularization:** Used dropout with a probability of 0.5 in FC6 and FC7 layers. Dropout helped prevent overfitting by randomly dropping out neurons during training, thereby improving the model's generalization ability.\n",
    "\n",
    "### 8. Parallel Computation:\n",
    "- **Utilization of Multiple GPUs:** AlexNet exploited the power of two GPUs for parallel computation, enabling faster training by distributing the workload across multiple processing units.\n",
    "\n",
    "### Impact:\n",
    "- The innovations introduced in AlexNet significantly improved accuracy in image classification tasks, marking a breakthrough in the field of computer vision.\n",
    "- These architectural advancements laid the groundwork for subsequent deeper and more complex neural network architectures, paving the way for the resurgence of deep learning in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discuss the role of convolutional layers, pooling layers and fully connected layers in AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the AlexNet architecture, convolutional layers, pooling layers, and fully connected layers each play distinct yet complementary roles in the process of feature extraction, dimensionality reduction, and high-level reasoning, respectively.\n",
    "\n",
    "### 1. Convolutional Layers:\n",
    "\n",
    "- **Role:** Convolutional layers perform feature extraction by applying convolution operations to input images. They detect various features such as edges, textures, and patterns.\n",
    "- **In AlexNet:** The architecture includes five convolutional layers (Conv1 to Conv5), employing different kernel sizes and depths.\n",
    "- **Key Aspects:**\n",
    "  - **Feature Hierarchies:** Each successive convolutional layer learns increasingly complex and abstract features by convolving over feature maps obtained from previous layers.\n",
    "  - **Large Kernels:** The initial layers (Conv1 and Conv2) use larger kernels like 11x11 and 5x5, capturing different levels of spatial information.\n",
    "\n",
    "### 2. Pooling Layers:\n",
    "\n",
    "- **Role:** Pooling layers reduce the spatial dimensions of the feature maps while retaining the most relevant information. They introduce translation invariance and help control the number of parameters.\n",
    "- **In AlexNet:** Three max pooling layers (MaxPool1, MaxPool2, MaxPool3) with a size of 3x3 and a stride of 2 are used.\n",
    "- **Key Aspects:**\n",
    "  - **Dimensionality Reduction:** Pooling layers downsample the feature maps, preserving important features and enhancing computational efficiency.\n",
    "  - **Feature Generalization:** By summarizing local information, pooling layers make the network more robust by focusing on the most activated features.\n",
    "\n",
    "### 3. Fully Connected Layers:\n",
    "\n",
    "- **Role:** Fully connected layers perform high-level reasoning and abstraction, capturing global dependencies and relationships between features extracted by convolutional layers.\n",
    "- **In AlexNet:** It consists of three fully connected layers: FC6, FC7, and FC8 (output layer).\n",
    "- **Key Aspects:**\n",
    "  - **High-Level Abstractions:** Fully connected layers connect all neurons, enabling the network to learn complex relationships in the data.\n",
    "  - **Classification:** The final fully connected layer (FC8) produces class probabilities corresponding to the ImageNet dataset's 1000 classes using the softmax activation function.\n",
    "\n",
    "### Overall Contribution:\n",
    "\n",
    "- **Convolutional Layers:** Extract hierarchical features.\n",
    "- **Pooling Layers:** Reduce spatial dimensions and emphasize important features.\n",
    "- **Fully Connected Layers:** Perform high-level reasoning and classification based on the learned representations.\n",
    "\n",
    "In AlexNet, these layers work cohesively, with convolutional layers extracting features, pooling layers reducing dimensions, and fully connected layers leveraging these features for accurate classification, collectively contributing to the network's success in image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet was primarily designed for large-scale image classification tasks, and MNIST, being a dataset of handwritten digits, might not be the best fit for AlexNet due to its architectural complexity and the nature of the dataset. However, for demonstration purposes, I'll guide you through implementing a simplified version of AlexNet using TensorFlow on the MNIST dataset. Please note that utilizing AlexNet for MNIST might not yield optimal results due to architectural differences and dataset characteristics.\n",
    "\n",
    "Here's a simplified version of AlexNet adapted for MNIST classification using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the MINS dataset\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the images to (num_samples, height, width, channels)\n",
    "train_images = tf.expand_dims(train_images, axis=-1)\n",
    "test_images = tf.expand_dims(test_images, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 241546 (943.54 KB)\n",
      "Trainable params: 241546 (943.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a simplified AlexNet model:\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 18s 36ms/step - loss: 0.2127 - accuracy: 0.9362 - val_loss: 0.0623 - val_accuracy: 0.9813\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0508 - accuracy: 0.9836 - val_loss: 0.0315 - val_accuracy: 0.9883\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0352 - accuracy: 0.9895 - val_loss: 0.0321 - val_accuracy: 0.9891\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0258 - accuracy: 0.9919 - val_loss: 0.0288 - val_accuracy: 0.9912\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 0.0212 - accuracy: 0.9936 - val_loss: 0.0283 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x11f5f718cd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0283 - accuracy: 0.9917\n",
      "Test accuracy: 99.17%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model performance\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Evaluation:\n",
    "* Accuracy Expectation: While AlexNet is over-engineered for MNIST due to its complexity, this simplified version might achieve moderate accuracy on the MNIST dataset (not comparable to its performance on more complex datasets).\n",
    "* Training Time: The training time might be relatively fast due to the reduced complexity compared to the original AlexNet.\n",
    "\n",
    "Please note that this adapted AlexNet for MNIST is a simplified version, and using it on the MNIST dataset might not fully leverage its capabilities. For MNIST, simpler architectures like a basic CNN might perform better. For more complex tasks and datasets, utilizing AlexNet or its modern variants might be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
